---
title: "DADA2 - MLSH ITS2 Plate 3 & 4"
output: html_notebook
---

```{r}
# if (!require("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# 
# BiocManager::install("dada2")

```


```{r}
ncores = 8
```

```{r}
library(dada2)
library(ShortRead)
library(Biostrings)
```


```{r}
seqDIR = '/project/mmicrobe/MLSH/ITS2/raw/lane1/'
path = seqDIR
list.files(path)

# #location of taxonomy database
# TrainingSet = '~/databases/silva_nr99_v138.1_train_set.fa.gz'
# SpeciesTraining = '~/databases/silva_species_assignment_v138.1.fa.gz'
# 
# # Sample data table
# SamTab = '/project/mmicrobe/Boyd/16S/BoydSampleData.csv'
# 
# # output folder
# OutFolder = '/project/mmicrobe/Boyd/16S/DADA2files'
```

# Collect files

```{r}
#Collect forward and reverse names in order
fnFs <- sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))
if(length(fnFs) != length(fnRs)) stop("Forward and reverse files do not match.")
```

# Identify primers
```{r}
FWD <- "AACTTTYRRCAAYGGATCWCT"  ## ITS2 5.8S forward primer sequence
REV <- "AGCCTCCGCTTATTGATATGCTTAART"  ## ITS2 ITS4Fun primer sequence
FSEQ = "TCTTTCCCTACACGACGCTCTTCCGATCT" # Forward sequencing primer
RSEQ = "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT" # Reverse sequencing primer
```

```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FSEQ.orients <- allOrients(FSEQ)
RSEQ.orients <- allOrients(RSEQ)
FWD.orients
REV.orients
FSEQ.orients
RSEQ.orients
```

```{r}
# if (file.exists(file.path(path, "filtn"))){
#   print(TRUE)
# } else {
#   #create new subdirectory
#   dir.create(file.path(path,"filtn"))
# }
```


```{r}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = 16)

fnFs.filtN[1]
fnRs.filtN[1]
```


## Check orientation and read through
```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(
  FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
  FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.filtN[[1]]), 
  REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.filtN[[1]]), 
  REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```
Checked a few samples.  The soil samples seem to have similar number of primer hits in the forward and reverse reads.  However, the root samples have much more in the forward than in the reverse.  Possibly from primer mismatch on non-target sequences?

Check for the sequencing adapters to see if there are similar counts
```{r}
rbind(
  FSEQ.ForwardReads = sapply(FSEQ.orients, primerHits, fn = fnFs.filtN[[1]]), 
  FSEQ.ReverseReads = sapply(FSEQ.orients,
    primerHits, fn = fnRs.filtN[[1]]), 
  RSEQ.ForwardReads = sapply(RSEQ.orients, primerHits,
    fn = fnFs.filtN[[1]]), 
  RSEQ.ReverseReads = sapply(RSEQ.orients, primerHits, fn = fnRs.filtN[[1]]))
```
Hmm, this suggests that all of the reverse reads have the reverse complement of the forward sequencing primer and all of the forward reads have the reverse complement of the reverse sequencing primer. 

# Execute cutadapt from bash script through ssh
The system command was not working properly, so used R to generate adapter and RC sequences and ran bash_cutadapt.sh script.  

```{r}
# cutadapt <- "/home/bryan.emmett/.local/bin/cutadapt"
# system2(command = cutadapt, args = "--version")

```
### Create directory for trimmed sequences
```{r}
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))
```

### Define primers
```{r}
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
FWD
REV
FWD.RC
REV.RC

R1.flags <- paste("-g", FWD, "-a", REV.RC)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
R1.flags
R2.flags
```

```{r}
fnRs.cut[1]
```
## Check cut adapt
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```
Check for the sequencing adapters to see if there are similar counts
```{r}
rbind(
  FSEQ.ForwardReads = sapply(FSEQ.orients, primerHits, fn = fnFs.cut[[1]]), 
  FSEQ.ReverseReads = sapply(FSEQ.orients,
    primerHits, fn = fnRs.cut[[1]]), 
  RSEQ.ForwardReads = sapply(RSEQ.orients, primerHits,
    fn = fnFs.cut[[1]]), 
  RSEQ.ReverseReads = sapply(RSEQ.orients, primerHits, fn = fnRs.cut[[1]]))
```
A lot of sequences remain with adapter sequences in some root samples. Additionally, many samples with short sequence.  After checking, it appears these very short fragments on the forward read are forward primer mismatches so they were not trimmed. The rest of the sequence has been trimmed by the cutadapt, leaving a small fragment. This happens differently based on each sample E.g. fnFs.cut[1] has many, number 2 has relatively few.  In the reverse orientation there are also sequencing primer sites early on in the sequence.  These are flanked by a long sequences, but likely sequencing junk.  Concluding that there are a lot of primer dimers in some samples. Need to re-run cut-adapt and include adapter sequences and filter for fragments smaller than a certain size.    
## Check after second cutadapt round

changing path to cut files to target new cutadapt2 folder

```{r}
path.cut2 <- file.path(path, "cutadapt2")
#if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut2, basename(fnFs))
fnRs.cut <- file.path(path.cut2, basename(fnRs))
```

```{r}
rbind(
  FSEQ.ForwardReads = sapply(FSEQ.orients, primerHits, fn = fnFs.cut[[1]]), 
  FSEQ.ReverseReads = sapply(FSEQ.orients,
    primerHits, fn = fnRs.cut[[1]]), 
  RSEQ.ForwardReads = sapply(RSEQ.orients, primerHits,
    fn = fnFs.cut[[1]]), 
  RSEQ.ReverseReads = sapply(RSEQ.orients, primerHits, fn = fnRs.cut[[1]]))
```



```{r}

# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut2, pattern = "_R1_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut2, pattern = "_R2_001.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_S")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
sample.names
```


# Plot quality profiles and forward and reverse reads
## Forward reads



```{r}
plotQualityProfile(fnFs.cut[c(1,22)])
```

Some files had zero lengths.  Re-running the cutadapt script with sequencing adapters and '-m 50' removed these sequences and short sequences.  Looks like files may be ready to go.  



## Reverse reads
```{r}
plotQualityProfile(fnRs.cut[c(1,22)])
```
* Forward reads look good, may not need to truncate
* Reverse reads also look very good, truncate to 225

## Last check

```{r}

readLines(fnFs.cut[1], n = 4)
readLines(fnRs.cut[1], n = 4)
#path
#fastqFs[1]
```
# Filter and trim

```{r}
filtFs <- file.path(path.cut2, "filtered", basename(cutFs))
filtRs <- file.path(path.cut2, "filtered", basename(cutRs))
```


```{r}
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), truncLen=c(250,200),truncQ = 2,
    minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = ncores)  # on windows, set multithread = FALSE
head(out)
```

```{r}
head(out)
saveRDS(out, file.path(OutFolder,'out.rds'))
```

```{r}
filtFs <- list.files(filtpathF, pattern=".fastq.gz", full.names = TRUE)
filtRs <- list.files(filtpathR, pattern=".fastq.gz", full.names = TRUE)
sample.names <- sapply(strsplit(basename(filtFs), "_L001_R1_001.fastq.gz"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
sample.namesR <- sapply(strsplit(basename(filtRs), "_L001_R2_001.fastq.gz"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
if(!identical(sample.names, sample.namesR)) stop("Forward and reverse files do not match.")
names(filtFs) <- sample.names
names(filtRs) <- sample.names
sample.names
```

# Learn errors

```{r}
set.seed(100)
# Learn forward error rates
errF <- learnErrors(filtFs, multithread=ncores)
# Learn reverse error rates
errR <- learnErrors(filtRs, multithread=ncores)
```

```{r}
plotErrors(errF)
errF_File = file.path(OutFolder,"errF.rds")
saveRDS(errF, errF_File)

errR_File = file.path(OutFolder,"errR.rds")
saveRDS(errR, errR_File)
```

# Sequence Inference


## Big data version
```{r}
# Sample inference and merger of paired-end reads
mergers <- vector("list", length(sample.names))
names(mergers) <- sample.names
for(sam in sample.names) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs[[sam]])
    ddF <- dada(derepF, err=errF, multithread=ncores)
    derepR <- derepFastq(filtRs[[sam]])
    ddR <- dada(derepR, err=errR, multithread=ncores)
    merger <- mergePairs(ddF, derepF, ddR, derepR)
    mergers[[sam]] <- merger
}
rm(derepF); rm(derepR)

mergers_File = file.path(OutFolder,"mergers.rds")
saveRDS(mergers, mergers_File)
```


# Checkpoint - read mergers file, construct sequence table and remove chimeras

```{r}
mergers_File = file.path(OutFolder,"mergers.rds")
mergers = readRDS(mergers_File)
seqtab <- makeSequenceTable(mergers)

outFile = file.path(OutFolder,"seqtab.rds")
saveRDS(seqtab, outFile)
```


```{r}
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
## Subset to proper length

```{r}
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(252,254)]

table(nchar(getSequences(seqtab2)))
```
## Remove chimeras

```{r}

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)

```

```{r}
dim(seqtab.nochim)
```

```{r}
outFile = file.path(OutFolder,"seqtab-nochim.rds")

saveRDS(seqtab.nochim, outFile)
```

# Checkpoint read seqtab.nochim
```{r}
CheckPoint = file.path(OutFolder,"seqtab-nochim.rds")

seqtab.nochim = readRDS(CheckPoint)
```

```{r}
paste("Fraction of reads not attributed to chimeras:", sum(seqtab.nochim)/sum(seqtab))
```

# Track reads through pipeline
```{r}
getN <- function(x) sum(getUniques(x))
#track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
track <- cbind(out, sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered",  "merged", "nonchim")
rownames(track) <- sample.names
head(track)

```


```{r}
track %>% as.data.frame() %>% mutate(sample = row.names(.)) %>% dplyr::arrange(desc(nonchim))
```
```{r}
outFile = file.path(OutFolder,'Tracking.txt')
write.table(track, file = outFile, sep = "\t")
```

## double check

```{r}
st2 = collapseNoMismatch(seqtab.nochim)
```

```{r}
dim(seqtab.nochim)
dim(st2)
```
```{r}
outFile = file.path(OutFolder,"st2.rds")

saveRDS(st2, outFile)
```

### Clean up memory
```{r}
rm(ddF, ddR, errF, errR, merger, mergers, out, seqtab, seqtab.nochim, seqtab2, track)
rm(st2)
```

# Checkpoint

```{r}
CheckPoint = file.path(OutFolder,"st2.rds")

seqtab.nochim = readRDS(CheckPoint)
```

# Assign Taxonomy
```{r}
 taxa <- assignTaxonomy(seqtab.nochim, TrainingSet, multithread=ncores)
```


```{r}
outFile = file.path(OutFolder,'taxa.rds')
outFile
saveRDS(taxa, outFile)
```

# Checkpoint read taxa file

```{r}
CheckPoint = file.path(OutFolder,"taxa.rds")
taxa = readRDS(CheckPoint)

```

```{r}
#inspect taxonomic assignment

taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```
```{r}
dim(taxa)
```
```{r}
names(seqtab.nochim)[1:4]
```

# Hand off to phyloseq

```{r}
# read sample data
samdf = read.delim(file = SamTab, header = TRUE, sep = ',')
head(samdf)
row.names(samdf) = samdf$X
samdf = samdf[,-1]
rownames(samdf)[1:4]
```
```{r}
dim(samdf)
```

```{r}
#rename taxa
taxa = cbind(taxa, row.names(taxa))

colnames(taxa)[7] = "Seq"
```

```{r}
rownames(seqtab.nochim)[1:50]
```
```{r}
rownames(seqtab.nochim) %>% length()
```

```{r}
rownames(samdf) %>% length
```
```{r}
toremove = setdiff(rownames(seqtab.nochim),rownames(samdf))
toremove
```
```{r}
allsamples = rownames(samdf)
allsamples = allsamples[!(allsamples %in% toremove)]
seqtab.nochim2 = seqtab.nochim[allsamples,]
seqtab.nochim2 %>% dim()
```
# Create phyloseq object
```{r}
ps <- phyloseq(otu_table(seqtab.nochim2, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps
```
```{r}
CheckOTU = taxa_names(ps)[150]
CheckOTU
```
```{r}
ps %>% prune_taxa(CheckOTU,.) %>% otu_table()
```
```{r}
sample_names(ps)[1:55]
```

## Rename taxa
```{r}
new.names <- paste0("ASV", seq(ntaxa(ps))) # Define new names ASV1, ASV2,
seqs <- taxa_names(ps) # Store sequences
names(seqs) <- new.names # Make map from ASV1 to full sequence
taxa_names(ps) <- new.names # Rename to human-friendly format
taxa_names(ps)[1:10]
```

```{r}
subset_taxa(ps, Seq == CheckOTU, TRUE) %>% 
    prune_samples("2-F04-E18R_S132",.) %>%
    otu_table()
```
* Sample sum matches

```{r}
taxa_sums(ps)[1:10]
```
## Save full object
```{r}
ps
saveRDS(ps, file = file.path(OutFolder, 'BoydPhyloseqFull.rds')) 
```

## Threshold to remove minor sequences: sum(x > 3) > 3

```{r}
tax_table(ps) %>% head
ps.thresh = filter_taxa(ps, function(x) sum(x > 3) > 3, TRUE)
ps.thresh
```

```{r}
rm(ps)
```

### Remove and save sequences from tax_table
*removing sequences will greatly speed up psmelt and subsequent operations
```{r}
Seqs_df = cbind(rownames(tax_table(ps.thresh)), tax_table(ps.thresh)[,'Seq'])
```

```{r}
colnames(Seqs_df)[1:2] = c("ASV", "Seq")
head(Seqs_df)
```

```{r}
taxa_df = tax_table(ps.thresh)[,1:6]
head(taxa_df)
```
```{r}
#save table of seqs
write.table(Seqs_df, file = file.path(OutFolder,'taxa_seqs.txt'), sep = '\t')
```

```{r}
# save fasta file of seqs
outfile = file.path(OutFolder,'seqs_thresh.fasta')

SeqNames = Seqs_df[,'ASV'] %>%
    as.list()
SeqNames[1:4]
seqs = Seqs_df[,'Seq'] %>% as.list()
seqs[1:4]
seqinr::write.fasta(sequences = as.list(seqs), names = SeqNames, file.out = outfile)
```

### Save thresholded phyloseq with simplified taxa table
```{r}
head(taxa_df)
```
```{r}
tax_table(ps.thresh) %>% head
```
```{r}
tax_table(ps.thresh) = taxa_df
head(tax_table(ps.thresh))
```

```{r}
ps.thresh
```

```{r}
outfile = file.path(OutFolder,'Boyd_phyloseq_thresh.rds')
saveRDS(ps.thresh, file = outfile)
```

# Next steps: sequences will be used to build a tree in a python script and then tree will be re-united with phyloseq object in subsequent notebook

```{r}
sessionInfo()
```
